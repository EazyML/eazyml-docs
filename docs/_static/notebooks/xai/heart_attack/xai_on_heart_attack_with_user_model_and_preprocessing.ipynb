{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57f452e-88d1-4318-8072-b984cae2879f",
   "metadata": {},
   "source": [
    "# EazyML Explainable AI Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6951e0-1c7e-4cc3-a93d-eae7960c99f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade eazyml-xai\n",
    "!pip install --upgrade eazyml-automl\n",
    "!pip install gdown python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300545a-a429-4c48-ab1e-c148ae2a710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from eazyml_xai import (\n",
    "    ez_init,\n",
    "    ez_explain\n",
    ")\n",
    "\n",
    "from eazyml import ez_display_df\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0417d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Initialize EazyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_init(access_key=os.getenv('EAZYML_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ec197-edc0-4e77-9020-e176d880a2bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Define Dataset Files and Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b879e24-2585-4897-b545-ba056cef3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download_folder(id='1EobxYR3pg_Z3Sd4sETfe4aJLAsT98fL2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097c5d5-ed97-4b4b-b8ae-c9819a7787f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the files that will be used by EazyML APIs\n",
    "train_file_path = os.path.join('data', 'Heart_Attack_traindata.csv')\n",
    "test_file_path  = os.path.join('data', 'Heart_Attack_testdata.csv')\n",
    "\n",
    "# The column name for outcome of interest\n",
    "outcome = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca4f2c-0fc9-4db3-993a-8488e81cb5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Implement Preprocessing Steps in a Preprocessor Class and Apply to the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbef529-2f50-4de2-8086-349963acec79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Implementing Preprocessing Steps within a Custom Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0d117-420e-4eaf-a29e-b92de323df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.numerical_imputer = SimpleImputer(strategy='mean')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.categorical_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.fitted = False  # To track whether preprocessing objects are fitted\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Split columns into numerical and categorical\n",
    "        self.numerical_columns = X.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_columns = X.select_dtypes(include=[object]).columns\n",
    "\n",
    "        # Fit transformers for features\n",
    "        self.numerical_imputer.fit(X[self.numerical_columns])\n",
    "        self.scaler.fit(X[self.numerical_columns])\n",
    "        self.categorical_encoder.fit(X[self.categorical_columns])\n",
    "\n",
    "        # Fit transformer for the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            self.label_encoder.fit(y)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "\n",
    "        # Apply transformations to numerical features\n",
    "        X_numerical = self.numerical_imputer.transform(X[self.numerical_columns])\n",
    "        X_numerical = self.scaler.transform(X_numerical)\n",
    "\n",
    "        # Apply transformations to categorical features\n",
    "        X_categorical = self.categorical_encoder.transform(X[self.categorical_columns])\n",
    "\n",
    "        # Get new column names for categorical features\n",
    "        categorical_feature_names = self.categorical_encoder.get_feature_names_out(self.categorical_columns)\n",
    "\n",
    "        # Combine transformed numerical and categorical data\n",
    "        X_transformed = np.hstack((X_numerical, X_categorical))\n",
    "\n",
    "        # Create a DataFrame with appropriate column names\n",
    "        all_feature_names = list(self.numerical_columns) + list(categorical_feature_names)\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "        # Transform the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            y_transformed = self.label_encoder.transform(y).flatten()\n",
    "            return X_transformed_df, y_transformed\n",
    "\n",
    "        return X_transformed_df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "    def inverse_transform_outcome(self, y):\n",
    "        \"\"\"\n",
    "        Revert the scaling of the target variable to its original scale.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "        y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "        return self.target_scaler.inverse_transform(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dea51-1d9e-45b3-ad0e-97f4ddfbb03a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2 Reading the Datasets and Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a86d75-885e-4c89-a70b-a5765a8792b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_columns = []\n",
    "\n",
    "# Reading Training Data\n",
    "train = pd.read_csv(train_file_path)\n",
    "train = train.drop(columns=discard_columns)\n",
    "\n",
    "# Reading Test Data\n",
    "test = pd.read_csv(test_file_path)\n",
    "test = test.drop(columns=discard_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe446a4-4885-4076-b35c-ce4458cba68c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3 Applying Preprocessing to the Training Data for Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9ae29-8bb9-4ec9-b66b-482f0d52e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train is your original training dataset\n",
    "y = train[outcome]\n",
    "X = train.drop(outcome, axis=1)\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "preprocessor = UnifiedPreprocessor()\n",
    "preprocessor.fit(X, y)\n",
    "\n",
    "# Transform the train dataset\n",
    "X_train_transformed, y_train_transformed = preprocessor.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52dd82-50b2-4bc1-a8e9-818839cb4549",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Training XGBoost Classifer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753f1e0-e2b7-4946-9725-e0714f5d797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
    "       colsample_bytree=0.8, gamma=0.5, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=20, min_child_weight=1.0, missing=np.nan,\n",
    "       n_estimators=10, n_jobs=1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=5.0, scale_pos_weight=1, seed=None, silent=False,\n",
    "       subsample=0.9)\n",
    "\n",
    "model = model_name.fit(X_train_transformed, y_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d67cb-8b9d-4fa5-8a73-d30c3924d4cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Get Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd6df5-1678-47e8-82e8-60ec50d8f5c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.1 Get Explanations for 5 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a728d22-ec90-4eda-bb44-90eb68f63e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'record_number': [1, 6, 7, 8, 9], 'preprocessor': preprocessor}\n",
    "response = ez_explain(train_file_path, outcome, test_file_path, model, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04056729-384f-4e4e-909b-439bc3ae5053",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.2 Display Explanation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70ecde-30c2-473e-9f07-042a1e78a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df = pd.DataFrame([i.values() for i in response['explanations']], columns=response['explanations'][0].keys())\n",
    "ez_display_df(ex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a03d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
