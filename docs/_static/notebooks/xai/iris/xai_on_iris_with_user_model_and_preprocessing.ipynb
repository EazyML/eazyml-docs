{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57f452e-88d1-4318-8072-b984cae2879f",
   "metadata": {},
   "source": [
    "# EazyML Explainable AI Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6951e0-1c7e-4cc3-a93d-eae7960c99f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade eazyml-xai\n",
    "!pip install --upgrade eazyml-automl\n",
    "!pip install gdown python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d300545a-a429-4c48-ab1e-c148ae2a710d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from eazyml_xai import (\n",
    "    ez_init,\n",
    "    ez_explain\n",
    ")\n",
    "\n",
    "from eazyml import ez_display_df\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6405d07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Initialize EazyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8057540e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'message': 'Initialized successfully. You may revoke your consent to sharing usage stats anytime. You have exclusive paid access.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ez_init(access_key=os.getenv('EAZYML_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ec197-edc0-4e77-9020-e176d880a2bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Define Dataset Files and Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download_folder(id='1DJtU6gI929GdEEZ3F_7w5LMnT90VvYI7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8097c5d5-ed97-4b4b-b8ae-c9819a7787f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the files that will be used by EazyML APIs\n",
    "train_file_path = os.path.join('data', 'IRIS_Train.csv')\n",
    "test_file_path  = os.path.join('data', 'IRIS_Test.csv')\n",
    "\n",
    "# The column name for outcome of interest\n",
    "outcome = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca4f2c-0fc9-4db3-993a-8488e81cb5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Implement Preprocessing Steps in a Preprocessor Class and Apply to the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d6c17-202e-489a-af76-e06cc100e584",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Implementing Preprocessing Steps within a Custom Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3386100-c890-4848-ac65-0a7940b026ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.numerical_imputer = SimpleImputer(strategy='mean')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.categorical_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.fitted = False  # To track whether preprocessing objects are fitted\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Split columns into numerical and categorical\n",
    "        self.numerical_columns = X.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_columns = X.select_dtypes(include=[object]).columns\n",
    "\n",
    "        # Fit transformers for features\n",
    "        self.numerical_imputer.fit(X[self.numerical_columns])\n",
    "        self.scaler.fit(X[self.numerical_columns])\n",
    "        self.categorical_encoder.fit(X[self.categorical_columns])\n",
    "\n",
    "        # Fit transformer for the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            self.label_encoder.fit(y)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "\n",
    "        # Apply transformations to numerical features\n",
    "        X_numerical = self.numerical_imputer.transform(X[self.numerical_columns])\n",
    "        X_numerical = self.scaler.transform(X_numerical)\n",
    "\n",
    "        # Apply transformations to categorical features\n",
    "        X_categorical = self.categorical_encoder.transform(X[self.categorical_columns])\n",
    "\n",
    "        # Get new column names for categorical features\n",
    "        categorical_feature_names = self.categorical_encoder.get_feature_names_out(self.categorical_columns)\n",
    "\n",
    "        # Combine transformed numerical and categorical data\n",
    "        X_transformed = np.hstack((X_numerical, X_categorical))\n",
    "\n",
    "        # Create a DataFrame with appropriate column names\n",
    "        all_feature_names = list(self.numerical_columns) + list(categorical_feature_names)\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "        # Transform the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            y_transformed = self.label_encoder.transform(y).flatten()\n",
    "            return X_transformed_df, y_transformed\n",
    "\n",
    "        return X_transformed_df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "    def inverse_transform_outcome(self, y):\n",
    "        \"\"\"\n",
    "        Revert the scaling of the target variable to its original scale.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "        y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "        return self.target_scaler.inverse_transform(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dea51-1d9e-45b3-ad0e-97f4ddfbb03a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2 Reading the Datasets and Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a86d75-885e-4c89-a70b-a5765a8792b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_columns = []\n",
    "\n",
    "# Reading Training Data\n",
    "train = pd.read_csv(train_file_path)\n",
    "train = train.drop(columns=discard_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ea9a9-0443-4055-973d-0db75ca38756",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3 Applying Preprocessing to the Training Data for Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2f605b-2e39-4ed6-bbcc-8c457b6495f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train is your original training dataset\n",
    "y = train[outcome]\n",
    "X = train.drop(outcome, axis=1)\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "preprocessor = UnifiedPreprocessor()\n",
    "preprocessor.fit(X, y)\n",
    "\n",
    "# Transform the train dataset\n",
    "X_train_transformed, y_train_transformed = preprocessor.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52dd82-50b2-4bc1-a8e9-818839cb4549",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Training Bagging Classifer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4753f1e0-e2b7-4946-9725-e0714f5d797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = BaggingClassifier(estimator=DecisionTreeClassifier(\n",
    "    class_weight=None, criterion='gini', max_depth=None,\n",
    "    max_features=None, max_leaf_nodes=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "    random_state=None, splitter='best'),\n",
    "    bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
    "    max_samples=1.0, n_estimators=5, n_jobs=None, oob_score=False,\n",
    "    random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "model = model_name.fit(X_train_transformed, y_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d67cb-8b9d-4fa5-8a73-d30c3924d4cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Get Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589164ad-2174-4e08-898b-2b15f4261cec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.1 Get Explanations for Top 2 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a728d22-ec90-4eda-bb44-90eb68f63e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'record_number': [1, 2], 'preprocessor': preprocessor}\n",
    "response = ez_explain(train_file_path, outcome, test_file_path, model, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04056729-384f-4e4e-909b-439bc3ae5053",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.2 Display Explanation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca70ecde-30c2-473e-9f07-042a1e78a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_de160 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_de160 td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_de160\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_de160_level0_col0\" class=\"col_heading level0 col0\" >record_numbers</th>\n",
       "      <th id=\"T_de160_level0_col1\" class=\"col_heading level0 col1\" >prediction</th>\n",
       "      <th id=\"T_de160_level0_col2\" class=\"col_heading level0 col2\" >explanation</th>\n",
       "      <th id=\"T_de160_level0_col3\" class=\"col_heading level0 col3\" >explainability_score</th>\n",
       "      <th id=\"T_de160_level0_col4\" class=\"col_heading level0 col4\" >local_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_de160_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_de160_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_de160_row0_col1\" class=\"data row0 col1\" >Iris-setosa</td>\n",
       "      <td id=\"T_de160_row0_col2\" class=\"data row0 col2\" > petal_length is -1.4 (that is less than or equal to -0.63)</td>\n",
       "      <td id=\"T_de160_row0_col3\" class=\"data row0 col3\" >95%</td>\n",
       "      <td id=\"T_de160_row0_col4\" class=\"data row0 col4\" >{'petal_length': 0.69, 'petal_width': 0.29}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de160_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_de160_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_de160_row1_col1\" class=\"data row1 col1\" >Iris-setosa</td>\n",
       "      <td id=\"T_de160_row1_col2\" class=\"data row1 col2\" > petal_length is -1.34 (that is less than or equal to -0.67)</td>\n",
       "      <td id=\"T_de160_row1_col3\" class=\"data row1 col3\" >96%</td>\n",
       "      <td id=\"T_de160_row1_col4\" class=\"data row1 col4\" >{'petal_length': 0.69, 'petal_width': 0.29}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_df = pd.DataFrame([i.values() for i in response['explanations']], columns=response['explanations'][0].keys())\n",
    "ez_display_df(ex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d45209-3aa5-4fc2-9522-8eef564240e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
