{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57f452e-88d1-4318-8072-b984cae2879f",
   "metadata": {},
   "source": [
    "# EazyML Explainable AI Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6951e0-1c7e-4cc3-a93d-eae7960c99f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade eazyml-xai\n",
    "!pip install --upgrade eazyml-automl\n",
    "!pip install gdown python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300545a-a429-4c48-ab1e-c148ae2a710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from eazyml_xai import (\n",
    "    ez_init,\n",
    "    ez_explain\n",
    ")\n",
    "\n",
    "from eazyml import ez_display_df\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6405d07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Initialize EazyML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739655c7-71bb-47f2-b11d-a8eacde2e592",
   "metadata": {},
   "source": [
    "The `ez_init` function uses the `EAZYML_ACCESS_KEY` environment variable for authentication. If the variable is not set, it defaults to a trial license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_init(access_key=os.getenv('EAZYML_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ec197-edc0-4e77-9020-e176d880a2bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Define Dataset Files and Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download_folder(id='1DJtU6gI929GdEEZ3F_7w5LMnT90VvYI7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097c5d5-ed97-4b4b-b8ae-c9819a7787f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the files that will be used by EazyML APIs\n",
    "train_file_path = os.path.join('data', 'IRIS_Train.csv')\n",
    "test_file_path  = os.path.join('data', 'IRIS_Test.csv')\n",
    "\n",
    "# The column name for outcome of interest\n",
    "outcome = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714efeec-f990-4d8e-8a00-fe6b62c76c92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91d594-ed23-489a-b50e-7e6d953b3fce",
   "metadata": {},
   "source": [
    "The dataset used in this notebook is the **Iris Dataset**, which is a well-known dataset in machine learning and statistics. It contains data about 150 iris flowers, with four features (sepal length, sepal width, petal length, and petal width) and the species of the flower (setosa, versicolor, or virginica).\n",
    "\n",
    "You can find more details and download the dataset from Kaggle using the following link:\n",
    "\n",
    "[Kaggle Iris Dataset](https://www.kaggle.com/datasets/uciml/iris)\n",
    "\n",
    "### Columns in the Dataset:\n",
    "- **sepal_length**: Sepal length of the flower (cm)\n",
    "- **sepal_width**: Sepal width of the flower (cm)\n",
    "- **petal_length**: Petal length of the flower (cm)\n",
    "- **petal_width**: Petal width of the flower (cm)\n",
    "- **species**: Species of the iris flower (setosa, versicolor, virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9ade7-e0a1-40ce-b8ec-58bea781228b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Display the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e36429-8b20-4f20-8943-e0e73cdec7c4",
   "metadata": {},
   "source": [
    "Below is a preview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc162818-30f8-49ba-abc8-4ad9eb8abae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the provided file\n",
    "train = pd.read_csv(train_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca4f2c-0fc9-4db3-993a-8488e81cb5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Implement Preprocessing Steps in a Preprocessor Class and Apply to the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d6c17-202e-489a-af76-e06cc100e584",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1 Implementing Preprocessing Steps within a Custom Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3386100-c890-4848-ac65-0a7940b026ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.numerical_imputer = SimpleImputer(strategy='mean')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.categorical_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.fitted = False  # To track whether preprocessing objects are fitted\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Split columns into numerical and categorical\n",
    "        self.numerical_columns = X.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_columns = X.select_dtypes(include=[object]).columns\n",
    "\n",
    "        # Fit transformers for features\n",
    "        self.numerical_imputer.fit(X[self.numerical_columns])\n",
    "        self.scaler.fit(X[self.numerical_columns])\n",
    "        self.categorical_encoder.fit(X[self.categorical_columns])\n",
    "\n",
    "        # Fit transformer for the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            self.label_encoder.fit(y)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "\n",
    "        # Apply transformations to numerical features\n",
    "        X_numerical = self.numerical_imputer.transform(X[self.numerical_columns])\n",
    "        X_numerical = self.scaler.transform(X_numerical)\n",
    "\n",
    "        # Apply transformations to categorical features\n",
    "        X_categorical = self.categorical_encoder.transform(X[self.categorical_columns])\n",
    "\n",
    "        # Get new column names for categorical features\n",
    "        categorical_feature_names = self.categorical_encoder.get_feature_names_out(self.categorical_columns)\n",
    "\n",
    "        # Combine transformed numerical and categorical data\n",
    "        X_transformed = np.hstack((X_numerical, X_categorical))\n",
    "\n",
    "        # Create a DataFrame with appropriate column names\n",
    "        all_feature_names = list(self.numerical_columns) + list(categorical_feature_names)\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "        # Transform the target variable (if provided)\n",
    "        if y is not None:\n",
    "            y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "            y_transformed = self.label_encoder.transform(y).flatten()\n",
    "            return X_transformed_df, y_transformed\n",
    "\n",
    "        return X_transformed_df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "    def inverse_transform_outcome(self, y):\n",
    "        \"\"\"\n",
    "        Revert the scaling of the target variable to its original scale.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor is not fitted yet. Call 'fit' first.\")\n",
    "        y = np.array(y).reshape(-1, 1)  # Reshape for scaler\n",
    "        return self.target_scaler.inverse_transform(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dea51-1d9e-45b3-ad0e-97f4ddfbb03a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.2 Reading the Datasets and Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a86d75-885e-4c89-a70b-a5765a8792b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_columns = []\n",
    "\n",
    "# Reading Training Data\n",
    "train = pd.read_csv(train_file_path)\n",
    "train = train.drop(columns=discard_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ea9a9-0443-4055-973d-0db75ca38756",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.3 Applying Preprocessing to the Training Data for Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f605b-2e39-4ed6-bbcc-8c457b6495f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train is your original training dataset\n",
    "y = train[outcome]\n",
    "X = train.drop(outcome, axis=1)\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "preprocessor = UnifiedPreprocessor()\n",
    "preprocessor.fit(X, y)\n",
    "\n",
    "# Transform the train dataset\n",
    "X_train_transformed, y_train_transformed = preprocessor.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52dd82-50b2-4bc1-a8e9-818839cb4549",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Training Bagging Classifer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753f1e0-e2b7-4946-9725-e0714f5d797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = BaggingClassifier(estimator=DecisionTreeClassifier(\n",
    "    class_weight=None, criterion='gini', max_depth=None,\n",
    "    max_features=None, max_leaf_nodes=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "    random_state=None, splitter='best'),\n",
    "    bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
    "    max_samples=1.0, n_estimators=5, n_jobs=None, oob_score=False,\n",
    "    random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "model = model_name.fit(X_train_transformed, y_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d67cb-8b9d-4fa5-8a73-d30c3924d4cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Get Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589164ad-2174-4e08-898b-2b15f4261cec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6.1 Get Explanations for Top 2 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a728d22-ec90-4eda-bb44-90eb68f63e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'record_number': [1, 2], 'preprocessor': preprocessor}\n",
    "response = ez_explain(train_file_path, outcome, test_file_path, model, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04056729-384f-4e4e-909b-439bc3ae5053",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6.2 Display Explanation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70ecde-30c2-473e-9f07-042a1e78a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df = pd.DataFrame([i.values() for i in response['explanations']], columns=response['explanations'][0].keys())\n",
    "ez_display_df(ex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d45209-3aa5-4fc2-9522-8eef564240e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
