{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c67bac",
   "metadata": {},
   "source": [
    "# EazyML Image XAI: Brain MRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa805d-7d32-4f56-81ef-bb845e9f504f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade eazyml-xai-image\n",
    "!pip install gdown python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882bd8e",
   "metadata": {},
   "source": [
    "## Define imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a92ee-fb68-4057-a74c-82664657c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import gdown\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from eazyml_xai_image import (\n",
    "    ez_init,\n",
    "    ez_xai_image_explain\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26ab04",
   "metadata": {},
   "source": [
    "### Initialize EazyML\n",
    "The `ez_init` function uses the `EAZYML_ACCESS_KEY` environment variable for authentication. If the variable is not set, it defaults to a trial license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6be357",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_init(access_key=os.getenv('EAZYML_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f170d",
   "metadata": {},
   "source": [
    "## Download image data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "gdown.download_folder(id='10BRxPpPlscUUIUEmKyQgAZenVzEoSrdk')\n",
    "# download model\n",
    "gdown.download_folder(id='16kcSmVus6gQvjhTTvaT3EVlGeyov-71q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92e37f-11d0-434c-b1da-6ba41159981d",
   "metadata": {},
   "source": [
    "## Load the model, Get the outputs, and Save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29d6c6-01ff-4641-8479-ca8a47ca1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model/unet_brain_mri_seg.hdf5\"\n",
    "\n",
    "def preprocess_image(img):\n",
    "    image_size = 256\n",
    "    return cv2.resize(img, (image_size,image_size))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01cc0a-8a14-4fe9-b61c-d1309a921d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "for j in range(11, 18):\n",
    "    filename = f\"data/TCGA_CS_4941_19960909_{j}.tif\"\n",
    "    predicted_filename = f\"data/kaggle_3m_test_{j}.csv\"\n",
    "    pred = model.predict(np.expand_dims(preprocess_image(cv2.imread(filename)), 0))[0,:,:,0]\n",
    "    pred = np.where(pred > 0.2, pred, 0)\n",
    "    cv2.imwrite(f\"data/kaggle_3m_test_op_{j}.jpg\", cv2.imread(filename))\n",
    "    np.savetxt(predicted_filename, pred, delimiter=\",\")\n",
    "    cv2.imwrite(predicted_filename.replace(\".csv\", \".jpg\"), cv2.imread(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48252ccd-9da4-4751-96b1-54e8b6983c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/kaggle_3m_test_11.jpg\"\n",
    "model_path = \"model/unet_brain_mri_seg.hdf5\"\n",
    "predicted_filename = \"data/kaggle_3m_test_11.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f3ea8-3232-4526-ab7b-ae0acdf83218",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 11\n",
    "filename = f\"data/TCGA_CS_4941_19960909_{j}.tif\"\n",
    "predicted_filename = f\"data/kaggle_3m_test_{j}.csv\"\n",
    "pred = model.predict(np.expand_dims(cv2.resize(cv2.imread(filename), (256, 256))/255, 0))\n",
    "np.sum(np.where(np.array(pred) < 0.8, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a5a71-7be2-426d-86f8-79ca3f280a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.imread(filename))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.array(pd.read_csv(predicted_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea313fcc-31cd-4ea0-aef2-625333e38537",
   "metadata": {},
   "source": [
    "# Confidence Score and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f1f38-2dcb-443c-8fc7-6c20437cf42e",
   "metadata": {},
   "source": [
    "### Defining any Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee01e3-84ef-4a58-96ee-036d138c4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    image_size = 256\n",
    "    return tf.image.resize(img, (image_size,image_size))/255\n",
    "\n",
    "required_functions = {\n",
    "    \"input_preprocess_fn\": preprocess_image\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6021c-32ae-4edc-a045-d423de65b9dc",
   "metadata": {},
   "source": [
    "### Default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6c29d-5bc8-4bc9-a539-b483f58e9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_xai_image_explain(filename, model_path, predicted_filename, options = {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945cc00-ac74-4ab1-806d-70bda5c3edff",
   "metadata": {},
   "source": [
    "## Confidence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150167b-dca3-4b0a-9579-fc8a97beadfa",
   "metadata": {},
   "source": [
    "### Weighted Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659db81-088e-4c45-9cf9-fcb1be94a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_xai_image_explain(filename, model_path, predicted_filename, {\"score_strategy\": \"weighted-moments\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82bbda-7111-43d9-8012-85b874a67297",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218625dc-cac4-4535-9804-76be19e56fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_xai_image_explain(filename, model_path, predicted_filename, {\"score_strategy\": \"log-info\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a45b0c-2704-49b2-a9a7-3f75fefc88ff",
   "metadata": {},
   "source": [
    "### Lime Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80496d4b-9c3f-49ea-93a7-c7bb37795a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ez_xai_image_explain(filename, model_path, predicted_filename, {\"score_strategy\": \"lime-confidence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9689a-83d0-429d-b06f-894cb193c1eb",
   "metadata": {},
   "source": [
    "## Image Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc31dc-a16f-41f9-9611-a9532c127590",
   "metadata": {},
   "source": [
    "### Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca6bb3-b223-4ef1-a319-57e1518d33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"xai_strategy\": \"gradcam\",\n",
    "    \"gradcam_layer\": \"conv2d_36\",\n",
    "    \"required_functions\": required_functions\n",
    "}\n",
    "resp = ez_xai_image_explain(filename, model_path, predicted_filename, options)\n",
    "print(resp)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.imread(resp['explanations']['explanation'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8e085-3d9a-4d1e-8c59-7433a194b8c9",
   "metadata": {},
   "source": [
    "### Highres Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bcc9e-9b20-4559-a232-dbc2e57bc45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"xai_strategy\": \"highres-gradcam\",\n",
    "    \"required_functions\": required_functions\n",
    "}\n",
    "resp = ez_xai_image_explain(filename, model_path, predicted_filename, options)\n",
    "print(resp)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.imread(resp['explanations']['explanation'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0893fb1-ae70-4bb7-90de-4ce98beec619",
   "metadata": {},
   "source": [
    "### Image Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed09d8b-bfb2-4dd8-9aaa-ad719e29a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"xai_strategy\": \"image-lime\",\n",
    "    \"required_functions\": required_functions\n",
    "}\n",
    "resp = ez_xai_image_explain(filename, model_path, predicted_filename, options)\n",
    "print(resp)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.imread(resp['explanations']['explanation'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e4eca-aa09-498c-93be-c6902fd4c1f4",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2241c-74c0-4a95-a15f-11bdc8514d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"data/kaggle_3m_test_11.jpg\",\n",
    "    \"data/kaggle_3m_test_12.jpg\",\n",
    "    \"data/kaggle_3m_test_13.jpg\",\n",
    "    \"data/kaggle_3m_test_14.jpg\",\n",
    "    \"data/kaggle_3m_test_15.jpg\",\n",
    "]\n",
    "model_path = \"model/unet_brain_mri_seg.hdf5\"\n",
    "predicted_filenames = [\n",
    "    \"data/kaggle_3m_test_11.csv\",\n",
    "    \"data/kaggle_3m_test_12.csv\",\n",
    "    \"data/kaggle_3m_test_13.csv\",\n",
    "    \"data/kaggle_3m_test_14.csv\",\n",
    "    \"data/kaggle_3m_test_15.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc192b-580d-4bee-b9ce-cf156f6fb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eazyml_xai_image import ez_image_active_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4d6ea-b12e-4379-aa45-6da3a9d1054f",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b771b-b66b-4748-b7df-6a693b0f4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_image_active_learning(filenames, model_path, predicted_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346d17a-8155-41be-8b76-7268b75fbccf",
   "metadata": {},
   "source": [
    "### Weighted Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef305385-8850-42c2-8c6d-f44d2764f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_image_active_learning(filenames, model_path, predicted_filenames, {\"score_strategy\": \"weighted-moments\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fbacd-d8dd-4c41-bcb4-3ac6d2fefce2",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a39c1-10eb-47de-9a9b-6238e6b6fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_image_active_learning(filenames, model_path, predicted_filenames, {\"score_strategy\": \"log-info\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059e8b5-6c68-4ba6-ad90-abb656921b14",
   "metadata": {},
   "source": [
    "### Lime Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd73b2-83b4-4faa-8dbf-a306d2f83b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ez_image_active_learning(filenames, model_path, predicted_filenames, {\"score_strategy\": \"lime-confidence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4b37e-44a1-4d66-b950-c0b85a56fcda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab084464-12b7-4e16-985a-75665687fd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c514d6-db24-4483-a937-2d3020019212",
   "metadata": {},
   "source": [
    "# Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699713ed-596d-4196-b664-7a8abca9ad18",
   "metadata": {},
   "source": [
    "### Preparing data for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14002b66-2d1b-430c-a5c8-56d7463b996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "tr_data = {\n",
    "    \"inputs\": [\n",
    "        \"data/kaggle_3m_test_11.jpg\",\n",
    "        \"data/kaggle_3m_test_12.jpg\",\n",
    "        \"data/kaggle_3m_test_13.jpg\",\n",
    "        \"data/kaggle_3m_test_14.jpg\",\n",
    "        \"data/kaggle_3m_test_15.jpg\",\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        \"data/kaggle_3m_test_op_11.jpg\",\n",
    "        \"data/kaggle_3m_test_op_12.jpg\",\n",
    "        \"data/kaggle_3m_test_op_13.jpg\",\n",
    "        \"data/kaggle_3m_test_op_14.jpg\",\n",
    "        \"data/kaggle_3m_test_op_15.jpg\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Model path\n",
    "model_path = \"model/unet_brain_mri_seg.hdf5\"\n",
    "\n",
    "# Validation data\n",
    "val_data = {\n",
    "    \"inputs\": [\n",
    "        \"data/kaggle_3m_test_11.jpg\",\n",
    "        \"data/kaggle_3m_test_12.jpg\",\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        \"data/kaggle_3m_test_op_11.jpg\",\n",
    "        \"data/kaggle_3m_test_op_12.jpg\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bd4c9-5022-40d3-a45d-88f67b93ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eazyml_xai_image import ez_image_model_evaluate, ez_image_online_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895f9f2-344e-46bd-b575-6ec53e6c9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = f\"./train_data.csv\"\n",
    "# model_path = f\"model_to_be_trained.h5\"\n",
    "new_model_path = \"trained_model.h5\"\n",
    "validation_data_path = f\"val_data.csv\"\n",
    "fields = [\"inputs\", \"labels\"]\n",
    "\n",
    "# Loss function for the image segmentation model\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.BinaryFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# Preprocessing function for inputs\n",
    "def preprocess_image(x):\n",
    "    x = tf.image.resize(x, (256, 256))\n",
    "    pre_ = sm.get_preprocessing(\"efficientnetb6\") \n",
    "    return pre_(x)\n",
    "\n",
    "# Preprocessing function for masks\n",
    "def preprocess_label(x):\n",
    "    mask = tf.cast(tf.where(x == 1, 1., 0.), dtype=tf.float32)\n",
    "    mask = tf.image.resize(mask, (256, 256))\n",
    "    return mask\n",
    "\n",
    "req_fns = {\n",
    "    \"input_preprocess_fn\": preprocess_image,\n",
    "    \"label_preprocess_fn\": preprocess_label,\n",
    "    \"loss_fn\": total_loss,\n",
    "    \"metric_fns\": {\n",
    "        \"metric_iou\": sm.metrics.IOUScore(threshold=0.5),\n",
    "        \"metric_fscore\":sm.metrics.FScore(threshold=0.5)\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_eval_options():\n",
    "    # Options to call ez_image_model_evaluate\n",
    "    eval_options = {}\n",
    "    eval_options[\"batch_size\"] = 1\n",
    "    eval_options[\"required_functions\"] = req_fns\n",
    "    eval_options[\"required_functions\"][\"path_to_save\"] = \"./pre_fns\"\n",
    "    return eval_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c091b-46f2-4356-964e-d92984502d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_options = {}\n",
    "tr_options[\"training_parameters\"] = {\n",
    "    \"batchsize\" : 1,\n",
    "    \"epochs\" : 2,\n",
    "    \"learning_rate\" : 1e-4,\n",
    "}\n",
    "tr_options[\"ol_strategy\"] = \"fine-tuning\"\n",
    "tr_options[\"tr_strategy\"] = \"normal\"\n",
    "tr_options[\"validation_data_path\"] = validation_data_path\n",
    "tr_options[\"new_model_path\"] = new_model_path\n",
    "tr_options[\"required_functions\"] = req_fns\n",
    "tr_options[\"log_file\"] = f\"suture_online_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b40024-14e1-44d9-8d29-08ae47553b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = {fields[0]: val_data[\"inputs\"], fields[1]: val_data[\"labels\"]}\n",
    "df = pd.DataFrame(list_dict)\n",
    "df.to_csv(validation_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d3e17-bfc0-48a8-bec6-267a19382e20",
   "metadata": {},
   "source": [
    "### Evaluate the current model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d44ee5-83ac-423a-af43-225a9ee8847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ez_image_model_evaluate(validation_data_path,\n",
    "                                   model_path,\n",
    "                                   get_eval_options())\n",
    "prev_val = response['eval_info']\n",
    "print(\"Initial weights evaluated on validation data.\")\n",
    "print(\"Loss: \", prev_val[0], \", IOU Score: \", prev_val[1], \", F-Score: \", prev_val[2], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d99e7-f115-4716-a6d5-1a184aad307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "list_dict = {fields[0]: tr_data[\"inputs\"], fields[1]: tr_data[\"labels\"]}\n",
    "df = pd.DataFrame(list_dict)\n",
    "df.to_csv(training_data_path)\n",
    "\n",
    "# Validation Data\n",
    "list_dict = {fields[0]: val_data[\"inputs\"], fields[1]: val_data[\"labels\"]}\n",
    "df = pd.DataFrame(list_dict)\n",
    "df.to_csv(validation_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903e5d0-1331-4889-916c-ddf0d445032f",
   "metadata": {},
   "source": [
    "### Training the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c2fcd-10f2-472d-9584-f45acc5a9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ez_image_online_learning(training_data_path,\n",
    "                        model_path,\n",
    "                        tr_options)\n",
    "print(\"Retraining completed.\")\n",
    "print(history, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56f9ac-d1a2-4265-a020-4a42a9e28b55",
   "metadata": {},
   "source": [
    "### Saving the new model as a h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e892d60-ee5f-4285-b72d-200152a390bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "model.load_weights(new_model_path)\n",
    "model.save(new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e237be-a5ce-48de-99c5-87f5fa2c2350",
   "metadata": {},
   "source": [
    "### Evaluating the new model on the validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910bafd-8e57-4aae-bfb4-937d4b5d64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ez_image_model_evaluate(validation_data_path,\n",
    "                                   new_model_path,\n",
    "                                   get_eval_options())\n",
    "prev_val = response['eval_info']\n",
    "print(\"Initial weights evaluated on validation data.\")\n",
    "print(\"Loss: \", prev_val[0], \", IOU Score: \", prev_val[1], \", F-Score: \", prev_val[2], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ae2ed-a573-41b7-8b59-41c822c26eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adf67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2865f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
